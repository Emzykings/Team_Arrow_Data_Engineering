# Team_Arrow_Data_Engineering
A data Engineering project by Team Arrow Datathon23
Agricultural Monitoring Systems

Overview
This dataset simulates real-time agricultural sensor data, providing a rich source of information to design and construct a robust data engineering solution. The primary objective of this project is to extract, transform, and load (ETL) the raw sensor data into structured fact and dimension tables within our snowflake data warehouse.

Context
Our business operates in the agricultural sector, where monitoring various environmental conditions and sensor data is crucial for optimizing crop yields and ensuring sustainable farming practices. This datathon is aligned with our commitment to leverage data engineering to streamline the processing and analysis of agricultural data.

Objectives
- ETL Pipeline Design: Participants must design and implement an ETL pipeline to process the raw sensor data efficiently.
- Data Warehouse Setup: Create fact and dimension tables within the data warehouse to store structured data.
- Data Quality Assurance: Ensure data integrity and quality during the ETL process.
- **Real-time Integration:** Consider real-time data ingestion to enable timely insights for farmers.
- **Scalability:** Develop a scalable solution to accommodate large volumes of sensor data.

Business Impact
The successful execution of this Project has tangible benefits for our business:

Easy of reporting: Structured data makes it easier for the data analysts create reports
Improved Decision-Making: Great reporting by extension make it easier to make informed decisions on irrigation, crop management, and resource allocation.
Efficiency Gains:** A well-designed ETL pipeline reduces manual data processing efforts, increasing operational efficiency.
Enhanced Crop Yields:** Timely access to accurate data aids in optimizing farming practices, leading to improved crop yields.
Sustainability:** Efficient resource use supports sustainable and environmentally responsible agriculture.

Conclusion
The Agricultural Monitoring dataset offers participants the opportunity to contribute to our goal of harnessing data engineering for smarter, more efficient agriculture. We look forward to innovative solutions that will help us transform raw sensor data into actionable insights, ultimately benefitting our farmers and the agricultural sector as a whole.
üõ†Ô∏è Tools & Resources
Snowflake or any other tool of your Choice
üë£ Process
Day 1: Data Ingestion and Schema Setup

Task 1: Data Ingestion
Load the raw data from all source tables (SensorDataRaw, WeatherDataRaw, SoilDataRaw, CropDataRaw, PestDataRaw, IrrigationDataRaw, LocationDataRaw) into staging tables within the Data Warehouse Schema.

Task 2: Schema Setup, Data Cleansing and Transformation
Create the schema for the data warehouse.
Cleanse and preprocess the data in the staging tables.
Handle missing values, data type conversions, and data quality checks.
Transform the data into a format suitable for fact and dimension Tables.

Day 2: Dimension Tables and Initial Fact Tables

Task 1: Create Dimension Tables
Based on the cleaned and transformed data, create dimension Tables for attributes such as Location, Time, Crop Type, Pest Type, and Irrigation Method.
Populate these dimension tables.

Task 2: Create Fact Tables
Identify fact Tables based on key performance indicators (KPIs) or metrics of you think would be useful for the data consumers.
Create and populate the initial fact tables using the cleaned and transformed data.

Task 3: Define Primary Keys and Foreign Keys
Define primary keys for dimension Tables.
Define foreign keys in fact Tables to establish relationships with dimension Tables.

Day 3: Additional Fact Tables and Optimization

Task 1: Indexing and Optimization
Implement appropriate indexing strategies to optimize query performance.
Analyze the query patterns and optimize the data model accordingly.
Perform data quality checks and address any anomalies.

Task 2: Documentation and Final Testing
Document the data model, including schema structure, table relationships, and data dictionary.
Conduct final testing to ensure the accuracy and consistency of the data.
Prepare the data warehouse for querying by data analysts and scientists.
